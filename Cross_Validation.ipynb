{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross_Validation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojni85hlQt83"
      },
      "source": [
        "*Bu colab dosyasındaki notlar Çağlar Hoca'nın izlememizi istediği [A Short Introduction to Entropy, Cross-Entropy and KL-Divergence videosundan](https://www.youtube.com/watch?v=ErfnhcEV1O8) anladıklarımı not aldığım bilgileri içermektedir*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yFOeAgiVa6w"
      },
      "source": [
        "Cross Entropy kaybı veya log kaybı(log loss), çıktısı 0 ile 1 arasında bir olasılık değeri olan bir sınıflandırma modellerinin performansını ölçmek için kullanılır. Neural Network'lerde birden fazla output olduğu için çıkan sonuçlar arasında işlem yapmamız gerekmektedir. Argmax'ın türevi kötü dolayısıyla backpropagation yapılamıyor. Dolayısıyla genellikle Softmax gibi aktivasyon fonksiyonları kullanılıyor (softmax'tan çıktıktan sonra tüm output'ların toplamı 1 olur.). \n",
        "\n",
        "Cross Entropy ise her bir sınıf için olan sonucun aktivasyon fonksiyonundan çıktıktan sonraki halinin negatif log'a ```(-log(x) = log(1/x))``` alınmasıdır.  Bunu her sınıf için hesapladıktan sonra çıkan değerleri toplayarak \"Total Cross Entropy\"yi buluruz. Amacımız her kayıp fonksiyonunda olduğu gibi bu değeri minimize etmektir. \n",
        "\n",
        "Hava durumu örneğinde ise gerçekleşebilecek olan ihtimallerin 2'ye göre log'unun alınması bize kaç bit gerekli olduğu bilgisini verir. Uncertanity Reduction ise olayın olasılığının tam tersidir. Negatif log alındıktan sonra her bir değeri kendi olma olasılığıyla çarparak ağırlıklı ortalama gerçekleştiriyoruz. Yani entropy aslında her gün hava durumunu öğrendiğinizde aldığımız ortalama bilgi miktarını ölçer.\n",
        "\n",
        "Cross Entropy'nin tercih edilmesinin sebebi ise loss değerini 0-1 arasında değil, sınırsız bir aralıkta gösterebiliyor olmasıdır. Ayrıca loss değerimiz çok büyük olduğunda backpropagation'da büyük adımlar atabilmemizi sağlarken tam tersi bir senaryoda da küçük adımlar atabilmemizi sağlamaktadır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtqXb2xHY4_z"
      },
      "source": [
        "**Referanslar**\n",
        "* [A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)\n",
        "* [Neural Networks Part 6: Cross Entropy](https://www.youtube.com/watch?v=6ArSys5qHAU)\n",
        "* [The basic concepts of entropy, cross entropy and KL divergence and the popular introduction of cross entropy loss function](https://chowdera.com/2021/02/20210202044024311d.html)\n",
        "* [Entropy, Cross-Entropy, and KL-Divergence Explained!](https://radiant-brushlands-42789.herokuapp.com/towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a)"
      ]
    }
  ]
}